{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 4. Word2Vec + CNN (GPU)\n\n# å°æ¯”å››ç¨®æ–¹æ³•\n| **æ–¹æ³•**                  | **ç‰¹å¾µå·¥ç¨‹**                | **æ¨¡å‹**                    | **æº–ç¢ºç‡é ä¼°** | **å„ªå‹¢**                                     | **åŠ£å‹¢**                                      | **GPU æ”¯æ´**            |\n|--------------------------|----------------------------|-----------------------------|----------------|---------------------------------------------|----------------------------------------------|-------------------------|\n| **TF-IDF + éš¨æ©Ÿæ£®æ—**      | ç¨€ç–ç‰¹å¾µè¡¨ç¤ºï¼Œè©é »èˆ‡é€†æ–‡æª”é »ç‡æ¬Šé‡ | éš¨æ©Ÿæ£®æ—                    | 75%-82%       | æ¨¡å‹ç©©å®šæ€§å¼·ï¼Œå°å™ªè²å’Œé«˜ç¶­æ•¸æ“šä¸æ•æ„Ÿ          | ç„¡æ³•è™•ç†éç·šæ€§æ¨¡å¼ï¼Œå°èªç¾©ä¿¡æ¯åˆ©ç”¨ä¸è¶³           | ä¸æ”¯æŒ                  |\n| **TF-IDF + Boosting**      | ç¨€ç–ç‰¹å¾µè¡¨ç¤ºï¼Œè©é »èˆ‡é€†æ–‡æª”é »ç‡æ¬Šé‡ | XGBoost æˆ– LightGBM         | 78%-85%       | æ“…é•·è™•ç†ç¨€ç–ç‰¹å¾µï¼Œå°éŒ¯åˆ†æ¨£æœ¬æœ‰è‰¯å¥½é©æ‡‰èƒ½åŠ›      | è¨“ç·´æˆæœ¬ç•¥é«˜ï¼Œéœ€èª¿åƒä»¥é”åˆ°æœ€ä½³æ•ˆæœ              | æ”¯æŒï¼ˆé¡¯è‘—åŠ é€Ÿï¼Œé©åˆå¤§æ•¸æ“šé›†ï¼‰|\n| **Word2Vec + éš¨æ©Ÿæ£®æ—**     | è©åµŒå…¥ï¼Œè¨ˆç®—å¥å‘é‡å¹³å‡å€¼       | éš¨æ©Ÿæ£®æ—                    | 72%-80%       | èƒ½çµåˆè©åµŒå…¥èªç¾©ç‰¹å¾µï¼Œæå‡èªç¾©æ•æ‰èƒ½åŠ›          | è©åµŒå…¥éœ€é è™•ç†ï¼Œéš¨æ©Ÿæ£®æ—å°éç·šæ€§èªç¾©çš„è™•ç†æœ‰é™     | ä¸æ”¯æŒ                  |\n| **Word2Vec + CNN**         | è©åµŒå…¥ï¼Œä¿ç•™èªåº            | å·ç©ç¥ç¶“ç¶²çµ¡                | 75%-85%       | æ•æ‰å±€éƒ¨èªç¾©ç‰¹å¾µï¼Œå°çŸ­æ–‡æœ¬æ•ˆæœä½³               | è¨“ç·´éœ€è¼ƒå¤šè³‡æºï¼Œå°é•·æ–‡æœ¬æ•ˆæœæœ‰é™               | æ”¯æŒï¼ˆé¡¯è‘—åŠ é€Ÿï¼‰         |\n| **BERT åµŒå…¥ + Transformer**| ä¸Šä¸‹æ–‡èªç¾©åµŒå…¥ï¼Œä¿ç•™å…¨å±€èªç¾© | é è¨“ç·´ BERT æ¨¡å‹             | 85%-90%       | èƒ½æ•æ‰ä¸Šä¸‹æ–‡èªç¾©ï¼Œåˆ†é¡æº–ç¢ºç‡æœ€é«˜               | è¨“ç·´å’Œæ¨ç†æˆæœ¬é«˜ï¼Œéœ€è¦å¤§é‡æ•¸æ“šå’Œè³‡æºæ”¯æŒ         | æ”¯æŒï¼ˆå¿…è¦ï¼Œå¦å‰‡é€Ÿåº¦è¼ƒæ…¢ï¼‰ |","metadata":{}},{"cell_type":"code","source":"#kaggle å‰ç½®ä½œæ¥­\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:25:38.330030Z","iopub.execute_input":"2024-11-24T02:25:38.330372Z","iopub.status.idle":"2024-11-24T02:25:40.229810Z","shell.execute_reply.started":"2024-11-24T02:25:38.330332Z","shell.execute_reply":"2024-11-24T02:25:40.228973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# å¼•å…¥å¿…è¦çš„åº«\nimport json\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:25:43.183901Z","iopub.execute_input":"2024-11-24T02:25:43.184598Z","iopub.status.idle":"2024-11-24T02:26:03.452013Z","shell.execute_reply.started":"2024-11-24T02:25:43.184562Z","shell.execute_reply":"2024-11-24T02:26:03.451254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# é€™éƒ¨åˆ†è«‹å°å¿ƒåŸ·è¡Œï¼Œé€™æ˜¯ \"æœªåšé\"  é›œè¨Šè™•ç†çš„ç¨‹å¼ç¢¼","metadata":{}},{"cell_type":"code","source":"# è³‡æ–™è®€å–èˆ‡è™•ç†\ndata = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nf.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# é€™éƒ¨åˆ†è«‹å°å¿ƒåŸ·è¡Œï¼Œé€™æ˜¯ \"åšé\" é›œè¨Šè™•ç†çš„ç¨‹å¼ç¢¼","metadata":{}},{"cell_type":"code","source":"import json\nimport re\nimport emoji\nimport pandas as pd\n\n# å®šç¾©è¡¨æƒ…ç¬¦è™Ÿåˆ°é—œéµå­—çš„æ˜ å°„å­—å…¸\nemoji_dict = {\n    'ğŸ˜‚': '[joy]',\n    'â¤ï¸': '[love]',\n    'ğŸ˜': '[adoration]',\n    'ğŸ˜­': '[cry]',\n    'â¤': '[care]',\n    'ğŸ˜Š': '[happy]',\n    'ğŸ™': '[pray]',\n    'ğŸ˜˜': '[kiss]',\n    'ğŸ’•': '[love_each_other]',\n    'ğŸ”¥': '[fire]',\n    'ğŸ˜©': '[weary]',\n    'ğŸ¤”': '[think]',\n    'ğŸ’¯': '[perfect]',\n    'ğŸ’™': '[loyalty]',\n    'ğŸ™„': '[annoyed]',\n    'ğŸ˜': '[happy]',\n    'ğŸ™Œ': '[celebrate]',\n    'ğŸ™ğŸ¾': '[pray]',\n    'ğŸ‘': '[approve]',\n    'ğŸ™ğŸ½': '[pray]'\n}\n\n# å®šç¾©æ¸…ç†æ¨æ–‡æ–‡æœ¬çš„å‡½æ•¸\ndef clean_tweet(text, emoji_dict):\n    # å°‡å®šç¾©çš„è¡¨æƒ…ç¬¦è™Ÿæ›¿æ›ç‚ºå°æ‡‰çš„é—œéµè©\n    for emj, keyword in emoji_dict.items():\n        text = text.replace(emj, keyword)\n    # ç§»é™¤å…¶é¤˜çš„è¡¨æƒ…ç¬¦è™Ÿ\n    text = emoji.replace_emoji(text, replace='')\n    # ç§»é™¤ <LH> æ¨™ç±¤\n    text = re.sub(r'<LH>', '', text)\n    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—å…ƒ\n    text = text.strip()\n    return text\n\n# è®€å–æ¨æ–‡è³‡æ–™\ndata1 = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data1.append(json.loads(line))\n\n# è™•ç†æ¯æ¢æ¨æ–‡ä¸¦å„²å­˜çµæœ\nprocessed_tweets = []\nfor entry in data1:\n    # æª¢æŸ¥ '_source' å’Œ 'tweet' æ˜¯å¦å­˜åœ¨æ–¼è¨˜éŒ„ä¸­\n    if '_source' in entry and 'tweet' in entry['_source']:\n        tweet = entry['_source']['tweet']\n        # æª¢æŸ¥ 'text' æ˜¯å¦å­˜åœ¨æ–¼ 'tweet' ä¸­\n        if 'text' in tweet:\n            tweet_text = tweet['text']\n            cleaned_text = clean_tweet(tweet_text, emoji_dict)\n            # å‰µå»ºè™•ç†å¾Œçš„æ¨æ–‡è¨˜éŒ„ï¼Œä¿ç•™ '_source' å’Œ 'tweet'\n            processed_tweet = {\n                '_source': {\n                    'tweet': tweet.copy()\n                }\n            }\n            # æ›´æ–°æ¸…ç†å¾Œçš„æ–‡æœ¬\n            processed_tweet['_source']['tweet']['text'] = cleaned_text\n            processed_tweets.append(processed_tweet)\n        else:\n            print(\"è¨˜éŒ„ä¸­ç¼ºå°‘ 'text' éµ\")\n    else:\n        print(\"è¨˜éŒ„ä¸­ç¼ºå°‘ '_source' æˆ– 'tweet' éµ\")\n\n# å°‡è™•ç†å¾Œçš„æ¨æ–‡è³‡æ–™å­˜å„²ç‚º JSON æª”æ¡ˆ\nwith open('/kaggle/working/tweets_DM_filtered_1.json', 'w') as outfile:\n    json.dump(processed_tweets, outfile, ensure_ascii=False, indent=4)\n\n\n\n# å°‡è™•ç†å¾Œçš„è³‡æ–™è½‰æ›ç‚º DataFrame\ndf_processed = pd.DataFrame(processed_tweets)\n\n# å®šç¾©è¼¸å‡ºç›®éŒ„å’Œæª”æ¡ˆåç¨±\noutput_dir = '/kaggle/working/'\noutput_file = 'tweets_DM_filtered_1.json'\n\n# æª¢æŸ¥ä¸¦å‰µå»ºç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# å°‡è™•ç†å¾Œçš„ DataFrame å„²å­˜ç‚º JSON æª”æ¡ˆ\noutput_path = os.path.join(output_dir, output_file)\ndf_processed.to_json(output_path, orient='records', lines=True, force_ascii=False)\n\n# æŠŠ /kaggle/working/tweets_DM_filtered_1.json è¼‰å…¥æˆç‚º data , å¯æ¨£å°±å¯ä»¥è·ŸåŸæœ¬ç¨‹å¼åˆä½µ\n\ndata = []\nwith open('/kaggle/working/tweets_DM_filtered_1.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nf.close()\nprint(\"ok\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:26:09.935923Z","iopub.execute_input":"2024-11-24T02:26:09.936453Z","iopub.status.idle":"2024-11-24T02:30:12.828062Z","shell.execute_reply.started":"2024-11-24T02:26:09.936421Z","shell.execute_reply":"2024-11-24T02:30:12.827157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ä¸‹é¢å°±éƒ½ä¸€æ¨£äº†","metadata":{}},{"cell_type":"code","source":"\n\nemotion = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\ndata_identification = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n\ndf = pd.DataFrame(data)\n_source = df['_source'].apply(lambda x: x['tweet'])\ndf = pd.DataFrame({\n    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n    'hashtags': _source.apply(lambda x: x['hashtags']),\n    'text': _source.apply(lambda x: x['text']),\n})\ndf = df.merge(data_identification, on='tweet_id', how='left')\n\ntrain_data = df[df['identification'] == 'train']\ntest_data = df[df['identification'] == 'test']\ntrain_data = train_data.merge(emotion, on='tweet_id', how='left')\ntrain_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n\n# è³‡æ–™æ¨£æœ¬åŒ–\ntrain_data_sample = train_data.sample(frac=0.3, random_state=42)\ny_train_data = train_data_sample['emotion']\nX_train_data = train_data_sample['text']\n\n# æ¨™ç±¤ç·¨ç¢¼\nle = LabelEncoder()\ny_train = le.fit_transform(y_train_data)\n\n# è¨“ç·´èˆ‡æ¸¬è©¦åˆ†å‰²\nX_train, X_val, y_train, y_val = train_test_split(X_train_data, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\n# æ–‡å­— Tokenization èˆ‡ Padding\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X_train)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_val_seq = tokenizer.texts_to_sequences(X_val)\n\nmax_len = 100\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\nX_val_pad = pad_sequences(X_val_seq, maxlen=max_len, padding='post')\n\n# Word2Vec åˆå§‹åŒ–ï¼ˆå¯é¸ï¼šå¦‚æœæœ‰é è¨“ç·´è©åµŒå…¥å‰‡æ›¿æ›ï¼‰\nembedding_dim = 100\nword_index = tokenizer.word_index\nvocab_size = min(len(word_index) + 1, 10000)\nembedding_matrix = np.random.uniform(-1, 1, (vocab_size, embedding_dim))\n\n# æ¨¡å‹æ§‹å»º (CNN)\nmodel = Sequential([\n    Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True),\n    Conv1D(filters=128, kernel_size=5, activation='relu'),\n    GlobalMaxPooling1D(),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(len(le.classes_), activation='softmax')\n])\n\n# æ¨¡å‹ç·¨è­¯\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# è¨“ç·´\nwith tf.device('/gpu:0'):  # å¼·åˆ¶ä½¿ç”¨ GPU\n    history = model.fit(\n        X_train_pad, y_train,\n        validation_data=(X_val_pad, y_val),\n        batch_size=128,\n        epochs=10,\n        verbose=1\n    )\n\n# æ¸¬è©¦è³‡æ–™é æ¸¬\ntest_texts = test_data['text']\ntest_seq = tokenizer.texts_to_sequences(test_texts)\nX_test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\ny_test_pred = model.predict(X_test_pad)\n\n# å°‡é æ¸¬çµæœè½‰æ›ç‚ºæ¨™ç±¤\ny_test_pred_labels = le.inverse_transform(np.argmax(y_test_pred, axis=1))\n\n# çµ„è£æäº¤æª”æ¡ˆ\nsubmission = pd.DataFrame({\n    'tweet_id': test_data['tweet_id'],\n    'emotion': y_test_pred_labels\n})\n\n# submission.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"ok\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:30:46.905858Z","iopub.execute_input":"2024-11-24T02:30:46.906254Z","iopub.status.idle":"2024-11-24T02:33:45.985594Z","shell.execute_reply.started":"2024-11-24T02:30:46.906222Z","shell.execute_reply":"2024-11-24T02:33:45.984682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:30:20.964292Z","iopub.execute_input":"2024-11-21T15:30:20.964583Z","iopub.status.idle":"2024-11-21T15:30:20.976945Z","shell.execute_reply.started":"2024-11-21T15:30:20.964556Z","shell.execute_reply":"2024-11-21T15:30:20.975975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. Word2Vec + CNN (GPU)\n# åˆ†æéƒ¨åˆ† : æ··æ·†çŸ©é™£ , Classification Report\n\n## æº–ç¢ºç‡ç´€éŒ„ :\n\n## è³‡æ–™æ²’éæ¿¾é\n- Accuracy: \n0.5514\n\n## è³‡æ–™æœ‰éæ¿¾é\n- Accuracy:\r\n0.5312","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 1. åˆ—å‡ºæ··æ·†çŸ©é™£\ny_val_pred = np.argmax(model.predict(X_val_pad), axis=1)\nconf_matrix = confusion_matrix(y_val, y_val_pred)\nprint(\"\\n\", \"----\", \"\\n\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# 2. ç•«å‡ºæ··æ·†çŸ©é™£\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title(\"Confusion Matrix Heatmap\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 3. å°å‡º Classification Report\nclass_report = classification_report(y_val, y_val_pred, target_names=le.classes_)\nprint(\"Classification Report:\")\nprint(class_report)\n\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 4. å°å‡º Accuracy\naccuracy = accuracy_score(y_val, y_val_pred)\nprint(\"Accuracy:\")\nprint(f\"{accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:34:33.447940Z","iopub.execute_input":"2024-11-24T02:34:33.448826Z","iopub.status.idle":"2024-11-24T02:34:39.391961Z","shell.execute_reply.started":"2024-11-24T02:34:33.448772Z","shell.execute_reply":"2024-11-24T02:34:39.391071Z"}},"outputs":[],"execution_count":null}]}