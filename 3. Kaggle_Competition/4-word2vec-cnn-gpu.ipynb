{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 4. Word2Vec + CNN (GPU)\n\n# 對比四種方法\n| **方法**                  | **特徵工程**                | **模型**                    | **準確率預估** | **優勢**                                     | **劣勢**                                      | **GPU 支援**            |\n|--------------------------|----------------------------|-----------------------------|----------------|---------------------------------------------|----------------------------------------------|-------------------------|\n| **TF-IDF + 隨機森林**      | 稀疏特徵表示，詞頻與逆文檔頻率權重 | 隨機森林                    | 75%-82%       | 模型穩定性強，對噪聲和高維數據不敏感          | 無法處理非線性模式，對語義信息利用不足           | 不支持                  |\n| **TF-IDF + Boosting**      | 稀疏特徵表示，詞頻與逆文檔頻率權重 | XGBoost 或 LightGBM         | 78%-85%       | 擅長處理稀疏特徵，對錯分樣本有良好適應能力      | 訓練成本略高，需調參以達到最佳效果              | 支持（顯著加速，適合大數據集）|\n| **Word2Vec + 隨機森林**     | 詞嵌入，計算句向量平均值       | 隨機森林                    | 72%-80%       | 能結合詞嵌入語義特徵，提升語義捕捉能力          | 詞嵌入需預處理，隨機森林對非線性語義的處理有限     | 不支持                  |\n| **Word2Vec + CNN**         | 詞嵌入，保留語序            | 卷積神經網絡                | 75%-85%       | 捕捉局部語義特徵，對短文本效果佳               | 訓練需較多資源，對長文本效果有限               | 支持（顯著加速）         |\n| **BERT 嵌入 + Transformer**| 上下文語義嵌入，保留全局語義 | 預訓練 BERT 模型             | 85%-90%       | 能捕捉上下文語義，分類準確率最高               | 訓練和推理成本高，需要大量數據和資源支持         | 支持（必要，否則速度較慢） |","metadata":{}},{"cell_type":"code","source":"#kaggle 前置作業\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:25:38.330030Z","iopub.execute_input":"2024-11-24T02:25:38.330372Z","iopub.status.idle":"2024-11-24T02:25:40.229810Z","shell.execute_reply.started":"2024-11-24T02:25:38.330332Z","shell.execute_reply":"2024-11-24T02:25:40.228973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 引入必要的庫\nimport json\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:25:43.183901Z","iopub.execute_input":"2024-11-24T02:25:43.184598Z","iopub.status.idle":"2024-11-24T02:26:03.452013Z","shell.execute_reply.started":"2024-11-24T02:25:43.184562Z","shell.execute_reply":"2024-11-24T02:26:03.451254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 這部分請小心執行，這是 \"未做過\"  雜訊處理的程式碼","metadata":{}},{"cell_type":"code","source":"# 資料讀取與處理\ndata = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nf.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 這部分請小心執行，這是 \"做過\" 雜訊處理的程式碼","metadata":{}},{"cell_type":"code","source":"import json\nimport re\nimport emoji\nimport pandas as pd\n\n# 定義表情符號到關鍵字的映射字典\nemoji_dict = {\n    '😂': '[joy]',\n    '❤️': '[love]',\n    '😍': '[adoration]',\n    '😭': '[cry]',\n    '❤': '[care]',\n    '😊': '[happy]',\n    '🙏': '[pray]',\n    '😘': '[kiss]',\n    '💕': '[love_each_other]',\n    '🔥': '[fire]',\n    '😩': '[weary]',\n    '🤔': '[think]',\n    '💯': '[perfect]',\n    '💙': '[loyalty]',\n    '🙄': '[annoyed]',\n    '😁': '[happy]',\n    '🙌': '[celebrate]',\n    '🙏🏾': '[pray]',\n    '👍': '[approve]',\n    '🙏🏽': '[pray]'\n}\n\n# 定義清理推文文本的函數\ndef clean_tweet(text, emoji_dict):\n    # 將定義的表情符號替換為對應的關鍵詞\n    for emj, keyword in emoji_dict.items():\n        text = text.replace(emj, keyword)\n    # 移除其餘的表情符號\n    text = emoji.replace_emoji(text, replace='')\n    # 移除 <LH> 標籤\n    text = re.sub(r'<LH>', '', text)\n    # 移除多餘的空白字元\n    text = text.strip()\n    return text\n\n# 讀取推文資料\ndata1 = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data1.append(json.loads(line))\n\n# 處理每條推文並儲存結果\nprocessed_tweets = []\nfor entry in data1:\n    # 檢查 '_source' 和 'tweet' 是否存在於記錄中\n    if '_source' in entry and 'tweet' in entry['_source']:\n        tweet = entry['_source']['tweet']\n        # 檢查 'text' 是否存在於 'tweet' 中\n        if 'text' in tweet:\n            tweet_text = tweet['text']\n            cleaned_text = clean_tweet(tweet_text, emoji_dict)\n            # 創建處理後的推文記錄，保留 '_source' 和 'tweet'\n            processed_tweet = {\n                '_source': {\n                    'tweet': tweet.copy()\n                }\n            }\n            # 更新清理後的文本\n            processed_tweet['_source']['tweet']['text'] = cleaned_text\n            processed_tweets.append(processed_tweet)\n        else:\n            print(\"記錄中缺少 'text' 鍵\")\n    else:\n        print(\"記錄中缺少 '_source' 或 'tweet' 鍵\")\n\n# 將處理後的推文資料存儲為 JSON 檔案\nwith open('/kaggle/working/tweets_DM_filtered_1.json', 'w') as outfile:\n    json.dump(processed_tweets, outfile, ensure_ascii=False, indent=4)\n\n\n\n# 將處理後的資料轉換為 DataFrame\ndf_processed = pd.DataFrame(processed_tweets)\n\n# 定義輸出目錄和檔案名稱\noutput_dir = '/kaggle/working/'\noutput_file = 'tweets_DM_filtered_1.json'\n\n# 檢查並創建目錄（如果不存在）\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# 將處理後的 DataFrame 儲存為 JSON 檔案\noutput_path = os.path.join(output_dir, output_file)\ndf_processed.to_json(output_path, orient='records', lines=True, force_ascii=False)\n\n# 把 /kaggle/working/tweets_DM_filtered_1.json 載入成為 data , 可樣就可以跟原本程式合併\n\ndata = []\nwith open('/kaggle/working/tweets_DM_filtered_1.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nf.close()\nprint(\"ok\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:26:09.935923Z","iopub.execute_input":"2024-11-24T02:26:09.936453Z","iopub.status.idle":"2024-11-24T02:30:12.828062Z","shell.execute_reply.started":"2024-11-24T02:26:09.936421Z","shell.execute_reply":"2024-11-24T02:30:12.827157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 下面就都一樣了","metadata":{}},{"cell_type":"code","source":"\n\nemotion = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\ndata_identification = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n\ndf = pd.DataFrame(data)\n_source = df['_source'].apply(lambda x: x['tweet'])\ndf = pd.DataFrame({\n    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n    'hashtags': _source.apply(lambda x: x['hashtags']),\n    'text': _source.apply(lambda x: x['text']),\n})\ndf = df.merge(data_identification, on='tweet_id', how='left')\n\ntrain_data = df[df['identification'] == 'train']\ntest_data = df[df['identification'] == 'test']\ntrain_data = train_data.merge(emotion, on='tweet_id', how='left')\ntrain_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n\n# 資料樣本化\ntrain_data_sample = train_data.sample(frac=0.3, random_state=42)\ny_train_data = train_data_sample['emotion']\nX_train_data = train_data_sample['text']\n\n# 標籤編碼\nle = LabelEncoder()\ny_train = le.fit_transform(y_train_data)\n\n# 訓練與測試分割\nX_train, X_val, y_train, y_val = train_test_split(X_train_data, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\n# 文字 Tokenization 與 Padding\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X_train)\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_val_seq = tokenizer.texts_to_sequences(X_val)\n\nmax_len = 100\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\nX_val_pad = pad_sequences(X_val_seq, maxlen=max_len, padding='post')\n\n# Word2Vec 初始化（可選：如果有預訓練詞嵌入則替換）\nembedding_dim = 100\nword_index = tokenizer.word_index\nvocab_size = min(len(word_index) + 1, 10000)\nembedding_matrix = np.random.uniform(-1, 1, (vocab_size, embedding_dim))\n\n# 模型構建 (CNN)\nmodel = Sequential([\n    Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True),\n    Conv1D(filters=128, kernel_size=5, activation='relu'),\n    GlobalMaxPooling1D(),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(len(le.classes_), activation='softmax')\n])\n\n# 模型編譯\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# 訓練\nwith tf.device('/gpu:0'):  # 強制使用 GPU\n    history = model.fit(\n        X_train_pad, y_train,\n        validation_data=(X_val_pad, y_val),\n        batch_size=128,\n        epochs=10,\n        verbose=1\n    )\n\n# 測試資料預測\ntest_texts = test_data['text']\ntest_seq = tokenizer.texts_to_sequences(test_texts)\nX_test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\ny_test_pred = model.predict(X_test_pad)\n\n# 將預測結果轉換為標籤\ny_test_pred_labels = le.inverse_transform(np.argmax(y_test_pred, axis=1))\n\n# 組裝提交檔案\nsubmission = pd.DataFrame({\n    'tweet_id': test_data['tweet_id'],\n    'emotion': y_test_pred_labels\n})\n\n# submission.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"ok\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:30:46.905858Z","iopub.execute_input":"2024-11-24T02:30:46.906254Z","iopub.status.idle":"2024-11-24T02:33:45.985594Z","shell.execute_reply.started":"2024-11-24T02:30:46.906222Z","shell.execute_reply":"2024-11-24T02:33:45.984682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T15:30:20.964292Z","iopub.execute_input":"2024-11-21T15:30:20.964583Z","iopub.status.idle":"2024-11-21T15:30:20.976945Z","shell.execute_reply.started":"2024-11-21T15:30:20.964556Z","shell.execute_reply":"2024-11-21T15:30:20.975975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. Word2Vec + CNN (GPU)\n# 分析部分 : 混淆矩陣 , Classification Report\n\n## 準確率紀錄 :\n\n## 資料沒過濾過\n- Accuracy: \n0.5514\n\n## 資料有過濾過\n- Accuracy:\r\n0.5312","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 1. 列出混淆矩陣\ny_val_pred = np.argmax(model.predict(X_val_pad), axis=1)\nconf_matrix = confusion_matrix(y_val, y_val_pred)\nprint(\"\\n\", \"----\", \"\\n\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# 2. 畫出混淆矩陣\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title(\"Confusion Matrix Heatmap\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 3. 印出 Classification Report\nclass_report = classification_report(y_val, y_val_pred, target_names=le.classes_)\nprint(\"Classification Report:\")\nprint(class_report)\n\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 4. 印出 Accuracy\naccuracy = accuracy_score(y_val, y_val_pred)\nprint(\"Accuracy:\")\nprint(f\"{accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:34:33.447940Z","iopub.execute_input":"2024-11-24T02:34:33.448826Z","iopub.status.idle":"2024-11-24T02:34:39.391961Z","shell.execute_reply.started":"2024-11-24T02:34:33.448772Z","shell.execute_reply":"2024-11-24T02:34:39.391071Z"}},"outputs":[],"execution_count":null}]}