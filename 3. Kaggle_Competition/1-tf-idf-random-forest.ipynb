{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. TF-IDF + Random Forest\n\n# å°æ¯”å››ç¨®æ–¹æ³•\r| **æ–¹æ³•**                  | **ç‰¹å¾µå·¥ç¨‹**                | **æ¨¡å‹**                    | **æº–ç¢ºç‡é ä¼°** | **å„ªå‹¢**                                     | **åŠ£å‹¢**                                      | **GPU æ”¯æ´**            |\r\n|--------------------------|----------------------------|-----------------------------|----------------|---------------------------------------------|----------------------------------------------|-------------------------|\r\n| **TF-IDF + éš¨æ©Ÿæ£®æ—**      | ç¨€ç–ç‰¹å¾µè¡¨ç¤ºï¼Œè©é »èˆ‡é€†æ–‡æª”é »ç‡æ¬Šé‡ | éš¨æ©Ÿæ£®æ—                    | 75%-82%       | æ¨¡å‹ç©©å®šæ€§å¼·ï¼Œå°å™ªè²å’Œé«˜ç¶­æ•¸æ“šä¸æ•æ„Ÿ          | ç„¡æ³•è™•ç†éç·šæ€§æ¨¡å¼ï¼Œå°èªç¾©ä¿¡æ¯åˆ©ç”¨ä¸è¶³           | ä¸æ”¯æŒ                  |\r\n| **TF-IDF + Boosting**      | ç¨€ç–ç‰¹å¾µè¡¨ç¤ºï¼Œè©é »èˆ‡é€†æ–‡æª”é »ç‡æ¬Šé‡ | XGBoost æˆ– LightGBM         | 78%-85%       | æ“…é•·è™•ç†ç¨€ç–ç‰¹å¾µï¼Œå°éŒ¯åˆ†æ¨£æœ¬æœ‰è‰¯å¥½é©æ‡‰èƒ½åŠ›      | è¨“ç·´æˆæœ¬ç•¥é«˜ï¼Œéœ€èª¿åƒä»¥é”åˆ°æœ€ä½³æ•ˆæœ              | æ”¯æŒï¼ˆé¡¯è‘—åŠ é€Ÿï¼Œé©åˆå¤§æ•¸æ“šé›†ï¼‰|\r\n| **Word2Vec + éš¨æ©Ÿæ£®æ—**     | è©åµŒå…¥ï¼Œè¨ˆç®—å¥å‘é‡å¹³å‡å€¼       | éš¨æ©Ÿæ£®æ—                    | 72%-80%       | èƒ½çµåˆè©åµŒå…¥èªç¾©ç‰¹å¾µï¼Œæå‡èªç¾©æ•æ‰èƒ½åŠ›          | è©åµŒå…¥éœ€é è™•ç†ï¼Œéš¨æ©Ÿæ£®æ—å°éç·šæ€§èªç¾©çš„è™•ç†æœ‰é™     | ä¸æ”¯æŒ                  |\r\n| **Word2Vec + CNN**         | è©åµŒå…¥ï¼Œä¿ç•™èªåº            | å·ç©ç¥ç¶“ç¶²çµ¡                | 75%-85%       | æ•æ‰å±€éƒ¨èªç¾©ç‰¹å¾µï¼Œå°çŸ­æ–‡æœ¬æ•ˆæœä½³               | è¨“ç·´éœ€è¼ƒå¤šè³‡æºï¼Œå°é•·æ–‡æœ¬æ•ˆæœæœ‰é™               | æ”¯æŒï¼ˆé¡¯è‘—åŠ é€Ÿï¼‰         |\r\n| **BERT åµŒå…¥ + Transformer**| ä¸Šä¸‹æ–‡èªç¾©åµŒå…¥ï¼Œä¿ç•™å…¨å±€èªç¾© | é è¨“ç·´ BERT æ¨¡å‹             | 85%-90%       | èƒ½æ•æ‰ä¸Šä¸‹æ–‡èªç¾©ï¼Œåˆ†é¡æº–ç¢ºç‡æœ€é«˜               | è¨“ç·´å’Œæ¨ç†æˆæœ¬é«˜ï¼Œéœ€è¦å¤§é‡æ•¸æ“šå’Œè³‡æºæ”¯æŒ         | æ”¯æŒï¼ˆå¿…è¦ï¼Œå¦å‰‡é€Ÿæ…¢ï¼‰ |\r     |\r\n","metadata":{}},{"cell_type":"code","source":"#kaggle å‰ç½®ä½œæ¥­\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T15:56:23.745299Z","iopub.execute_input":"2024-11-23T15:56:23.745829Z","iopub.status.idle":"2024-11-23T15:56:23.761140Z","shell.execute_reply.started":"2024-11-23T15:56:23.745791Z","shell.execute_reply":"2024-11-23T15:56:23.759518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# å¼•å…¥å¿…è¦çš„åº«\nimport json\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T15:56:26.428511Z","iopub.execute_input":"2024-11-23T15:56:26.429006Z","iopub.status.idle":"2024-11-23T15:56:26.435788Z","shell.execute_reply.started":"2024-11-23T15:56:26.428966Z","shell.execute_reply":"2024-11-23T15:56:26.434408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# é€™éƒ¨åˆ†è«‹å°å¿ƒåŸ·è¡Œï¼Œé€™æ˜¯ \"æœªåšé\"  é›œè¨Šè™•ç†çš„ç¨‹å¼ç¢¼","metadata":{}},{"cell_type":"code","source":"# ======== è³‡æ–™è®€å–èˆ‡æ•´ç† ========\n# 1. è®€å– JSON æ ¼å¼çš„æ¨æ–‡æ•¸æ“š\ndata = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nf.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# é€™éƒ¨åˆ†è«‹å°å¿ƒåŸ·è¡Œï¼Œé€™æ˜¯ \"åšé\" é›œè¨Šè™•ç†çš„ç¨‹å¼ç¢¼","metadata":{}},{"cell_type":"code","source":"import json\nimport re\nimport emoji\nimport pandas as pd\n\n# å®šç¾©è¡¨æƒ…ç¬¦è™Ÿåˆ°é—œéµå­—çš„æ˜ å°„å­—å…¸\nemoji_dict = {\n    'ğŸ˜‚': '[joy]',\n    'â¤ï¸': '[love]',\n    'ğŸ˜': '[adoration]',\n    'ğŸ˜­': '[cry]',\n    'â¤': '[care]',\n    'ğŸ˜Š': '[happy]',\n    'ğŸ™': '[pray]',\n    'ğŸ˜˜': '[kiss]',\n    'ğŸ’•': '[love_each_other]',\n    'ğŸ”¥': '[fire]',\n    'ğŸ˜©': '[weary]',\n    'ğŸ¤”': '[think]',\n    'ğŸ’¯': '[perfect]',\n    'ğŸ’™': '[loyalty]',\n    'ğŸ™„': '[annoyed]',\n    'ğŸ˜': '[happy]',\n    'ğŸ™Œ': '[celebrate]',\n    'ğŸ™ğŸ¾': '[pray]',\n    'ğŸ‘': '[approve]',\n    'ğŸ™ğŸ½': '[pray]'\n}\n\n# å®šç¾©æ¸…ç†æ¨æ–‡æ–‡æœ¬çš„å‡½æ•¸\ndef clean_tweet(text, emoji_dict):\n    # å°‡å®šç¾©çš„è¡¨æƒ…ç¬¦è™Ÿæ›¿æ›ç‚ºå°æ‡‰çš„é—œéµè©\n    for emj, keyword in emoji_dict.items():\n        text = text.replace(emj, keyword)\n    # ç§»é™¤å…¶é¤˜çš„è¡¨æƒ…ç¬¦è™Ÿ\n    text = emoji.replace_emoji(text, replace='')\n    # ç§»é™¤ <LH> æ¨™ç±¤\n    text = re.sub(r'<LH>', '', text)\n    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—å…ƒ\n    text = text.strip()\n    return text\n\n# è®€å–æ¨æ–‡è³‡æ–™\ndata1 = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data1.append(json.loads(line))\n\n# è™•ç†æ¯æ¢æ¨æ–‡ä¸¦å„²å­˜çµæœ\nprocessed_tweets = []\nfor entry in data1:\n    # æª¢æŸ¥ '_source' å’Œ 'tweet' æ˜¯å¦å­˜åœ¨æ–¼è¨˜éŒ„ä¸­\n    if '_source' in entry and 'tweet' in entry['_source']:\n        tweet = entry['_source']['tweet']\n        # æª¢æŸ¥ 'text' æ˜¯å¦å­˜åœ¨æ–¼ 'tweet' ä¸­\n        if 'text' in tweet:\n            tweet_text = tweet['text']\n            cleaned_text = clean_tweet(tweet_text, emoji_dict)\n            # å‰µå»ºè™•ç†å¾Œçš„æ¨æ–‡è¨˜éŒ„ï¼Œä¿ç•™ '_source' å’Œ 'tweet'\n            processed_tweet = {\n                '_source': {\n                    'tweet': tweet.copy()\n                }\n            }\n            # æ›´æ–°æ¸…ç†å¾Œçš„æ–‡æœ¬\n            processed_tweet['_source']['tweet']['text'] = cleaned_text\n            processed_tweets.append(processed_tweet)\n        else:\n            print(\"è¨˜éŒ„ä¸­ç¼ºå°‘ 'text' éµ\")\n    else:\n        print(\"è¨˜éŒ„ä¸­ç¼ºå°‘ '_source' æˆ– 'tweet' éµ\")\n\n# å°‡è™•ç†å¾Œçš„æ¨æ–‡è³‡æ–™å­˜å„²ç‚º JSON æª”æ¡ˆ\nwith open('/kaggle/working/tweets_DM_filtered_1.json', 'w') as outfile:\n    json.dump(processed_tweets, outfile, ensure_ascii=False, indent=4)\n\n\n\n# å°‡è™•ç†å¾Œçš„è³‡æ–™è½‰æ›ç‚º DataFrame\ndf_processed = pd.DataFrame(processed_tweets)\n\n# å®šç¾©è¼¸å‡ºç›®éŒ„å’Œæª”æ¡ˆåç¨±\noutput_dir = '/kaggle/working/'\noutput_file = 'tweets_DM_filtered_1.json'\n\n# æª¢æŸ¥ä¸¦å‰µå»ºç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# å°‡è™•ç†å¾Œçš„ DataFrame å„²å­˜ç‚º JSON æª”æ¡ˆ\noutput_path = os.path.join(output_dir, output_file)\ndf_processed.to_json(output_path, orient='records', lines=True, force_ascii=False)\n\n# æŠŠ /kaggle/working/tweets_DM_filtered_1.json è¼‰å…¥æˆç‚º data , å¯æ¨£å°±å¯ä»¥è·ŸåŸæœ¬ç¨‹å¼åˆä½µ\n\ndata = []\nwith open('/kaggle/working/tweets_DM_filtered_1.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nf.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T15:56:32.119086Z","iopub.execute_input":"2024-11-23T15:56:32.119445Z","iopub.status.idle":"2024-11-23T16:01:55.961281Z","shell.execute_reply.started":"2024-11-23T15:56:32.119413Z","shell.execute_reply":"2024-11-23T16:01:55.960077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ä»¥ä¸‹å°±éƒ½è·Ÿä¹‹å‰ä¸€æ¨£äº†","metadata":{}},{"cell_type":"code","source":"# 2. è®€å–å…¶ä»– CSV æ–‡ä»¶\nemotion = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\ndata_identification = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n\n# ======== æ•¸æ“šè™•ç† ========\n# 1. å¾ JSON æ•¸æ“šæå– tweet_id, hashtags, å’Œæ–‡æœ¬å…§å®¹\ndf = pd.DataFrame(data)\n_source = df['_source'].apply(lambda x: x['tweet'])\ndf = pd.DataFrame({\n    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n    'hashtags': _source.apply(lambda x: x['hashtags']),\n    'text': _source.apply(lambda x: x['text']),\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T16:10:48.050793Z","iopub.execute_input":"2024-11-23T16:10:48.051320Z","iopub.status.idle":"2024-11-23T16:11:01.464818Z","shell.execute_reply.started":"2024-11-23T16:10:48.051281Z","shell.execute_reply":"2024-11-23T16:11:01.463448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# 2. åˆä½µè³‡æ–™é›†æ¨™ç±¤ (train/test)\ndf = df.merge(data_identification, on='tweet_id', how='left')\n\n# 3. å°‡è³‡æ–™åˆ†ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†\ntrain_data = df[df['identification'] == 'train']\ntest_data = df[df['identification'] == 'test']\n\n# 4. åˆä½µæƒ…ç·’æ¨™ç±¤åˆ°è¨“ç·´è³‡æ–™ä¸­\ntrain_data = train_data.merge(emotion, on='tweet_id', how='left')\n\n# 5. ç§»é™¤è¨“ç·´é›†ä¸­é‡è¤‡çš„æ–‡æœ¬\ntrain_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n\n# ======== è¨“ç·´æ•¸æ“šæº–å‚™ ========\n# 1. æŠ½å–éƒ¨åˆ†è¨“ç·´æ•¸æ“šï¼ˆ30% æŠ½æ¨£ï¼‰\ntrain_data_sample = train_data.sample(frac=0.3, random_state=42)\n\n# 2. æå–ç›®æ¨™è®Šæ•¸ (æƒ…ç·’) å’Œç‰¹å¾µè®Šæ•¸ (æ–‡æœ¬)\ny_train_data = train_data_sample['emotion']\nX_train_data = train_data_sample.drop(['tweet_id', 'emotion', 'identification', 'hashtags'], axis=1)\n\n# 3. è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†åŠƒåˆ† (åˆ†å±¤æŠ½æ¨£ç¢ºä¿é¡åˆ¥åˆ†ä½ˆä¸€è‡´)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_train_data, y_train_data, test_size=0.2, random_state=42, stratify=y_train_data\n)\n\n# ======== æ–‡æœ¬ç‰¹å¾µè½‰æ› ========\n# 1. ä½¿ç”¨ TF-IDF å°‡æ–‡æœ¬è½‰æ›ç‚ºæ•¸å€¼ç‰¹å¾µ\ntfidf = TfidfVectorizer(max_features=500)\nX = tfidf.fit_transform(X_train['text']).toarray()\nX_test = tfidf.transform(X_test['text'])\n\n# ======== æ¨™ç±¤ç·¨ç¢¼ ========\n# 1. å°‡æƒ…ç·’é¡åˆ¥è½‰æ›ç‚ºæ•¸å€¼å‹\nle = LabelEncoder()\ny = le.fit_transform(y_train)\ny_test = le.transform(y_test)\n\n# ======== æ¨¡å‹è¨“ç·´ ========\n# 1. è¨“ç·´éš¨æ©Ÿæ£®æ—åˆ†é¡å™¨\nclf = RandomForestClassifier()\nclf.fit(X, y)\n\n# ======== æ¨¡å‹è©•ä¼° ========\n# 1. é æ¸¬æ¸¬è©¦é›†\ny_pred = clf.predict(X_test)\n\n# 2. è¨ˆç®—æº–ç¢ºç‡\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n\n# ======== æ¸¬è©¦æ•¸æ“šé æ¸¬ ========\n# 1. æº–å‚™æ¸¬è©¦æ•¸æ“šçš„æ–‡æœ¬ç‰¹å¾µ\nX_test_data = test_data.drop(['tweet_id', 'identification', 'hashtags'], axis=1)\nX_test_data = tfidf.transform(X_test_data['text']).toarray()\n\n# 2. é æ¸¬æ¸¬è©¦æ•¸æ“šæƒ…ç·’\ny_test_pred = clf.predict(X_test_data)\n\n# ======== çµæœè¼¸å‡º ========\n# 1. å°‡æ•¸å€¼æ¨™ç±¤è½‰æ›å›æ–‡å­—æ¨™ç±¤\ny_pred_labels = le.inverse_transform(y_test_pred)\n\n# 2. ä¿å­˜æäº¤æª”æ¡ˆ\nsubmission = pd.DataFrame({\n    'tweet_id': test_data['tweet_id'],\n    'emotion': y_pred_labels\n})\n\n# å…ˆä¸ç”¢ç”Ÿ submission\n# submission.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T16:11:08.923409Z","iopub.execute_input":"2024-11-23T16:11:08.924496Z","iopub.status.idle":"2024-11-23T16:27:35.031860Z","shell.execute_reply.started":"2024-11-23T16:11:08.924450Z","shell.execute_reply":"2024-11-23T16:27:35.030052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T16:28:59.270037Z","iopub.execute_input":"2024-11-23T16:28:59.270524Z","iopub.status.idle":"2024-11-23T16:28:59.284416Z","shell.execute_reply.started":"2024-11-23T16:28:59.270481Z","shell.execute_reply":"2024-11-23T16:28:59.283268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# åˆ†æéƒ¨åˆ† : æ··æ·†çŸ©é™£ , Classification Report\n\n# 1. TF-IDF + Randon_forest\n## æº–ç¢ºç‡ç´€éŒ„ :\n### æœªåšé›œè¨Šè™•ç†å‰\n- Accuracy: 0.4799\n- Accuracy: 0.4808\n\n### åšéé›œè¨Šè™•ç†å¾Œ\n- Accuracy: 0.4717\n- Accuracy: 0.4708","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# ======== åˆ†æéƒ¨åˆ† ========\n\n# 1. åˆ—å‡ºæ··æ·†çŸ©é™£\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 2. ç•«å‡ºæ··æ·†çŸ©é™£\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 3 . Classification Report\nclass_report = classification_report(y_test, y_pred, target_names=le.classes_)\nprint(\"Classification Report:\")\nprint(class_report)\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 4. å°å‡º Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"\\n\", \"----\", \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T16:29:04.034011Z","iopub.execute_input":"2024-11-23T16:29:04.034478Z","iopub.status.idle":"2024-11-23T16:29:04.726109Z","shell.execute_reply.started":"2024-11-23T16:29:04.034440Z","shell.execute_reply":"2024-11-23T16:29:04.724556Z"}},"outputs":[],"execution_count":null}]}