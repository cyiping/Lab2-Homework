{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. TF-IDF + Random Forest\n\n# 對比四種方法\r| **方法**                  | **特徵工程**                | **模型**                    | **準確率預估** | **優勢**                                     | **劣勢**                                      | **GPU 支援**            |\r\n|--------------------------|----------------------------|-----------------------------|----------------|---------------------------------------------|----------------------------------------------|-------------------------|\r\n| **TF-IDF + 隨機森林**      | 稀疏特徵表示，詞頻與逆文檔頻率權重 | 隨機森林                    | 75%-82%       | 模型穩定性強，對噪聲和高維數據不敏感          | 無法處理非線性模式，對語義信息利用不足           | 不支持                  |\r\n| **TF-IDF + Boosting**      | 稀疏特徵表示，詞頻與逆文檔頻率權重 | XGBoost 或 LightGBM         | 78%-85%       | 擅長處理稀疏特徵，對錯分樣本有良好適應能力      | 訓練成本略高，需調參以達到最佳效果              | 支持（顯著加速，適合大數據集）|\r\n| **Word2Vec + 隨機森林**     | 詞嵌入，計算句向量平均值       | 隨機森林                    | 72%-80%       | 能結合詞嵌入語義特徵，提升語義捕捉能力          | 詞嵌入需預處理，隨機森林對非線性語義的處理有限     | 不支持                  |\r\n| **Word2Vec + CNN**         | 詞嵌入，保留語序            | 卷積神經網絡                | 75%-85%       | 捕捉局部語義特徵，對短文本效果佳               | 訓練需較多資源，對長文本效果有限               | 支持（顯著加速）         |\r\n| **BERT 嵌入 + Transformer**| 上下文語義嵌入，保留全局語義 | 預訓練 BERT 模型             | 85%-90%       | 能捕捉上下文語義，分類準確率最高               | 訓練和推理成本高，需要大量數據和資源支持         | 支持（必要，否則速慢） |\r     |\r\n","metadata":{}},{"cell_type":"code","source":"#kaggle 前置作業\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T15:56:23.745299Z","iopub.execute_input":"2024-11-23T15:56:23.745829Z","iopub.status.idle":"2024-11-23T15:56:23.761140Z","shell.execute_reply.started":"2024-11-23T15:56:23.745791Z","shell.execute_reply":"2024-11-23T15:56:23.759518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 引入必要的庫\nimport json\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T15:56:26.428511Z","iopub.execute_input":"2024-11-23T15:56:26.429006Z","iopub.status.idle":"2024-11-23T15:56:26.435788Z","shell.execute_reply.started":"2024-11-23T15:56:26.428966Z","shell.execute_reply":"2024-11-23T15:56:26.434408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 這部分請小心執行，這是 \"未做過\"  雜訊處理的程式碼","metadata":{}},{"cell_type":"code","source":"# ======== 資料讀取與整理 ========\n# 1. 讀取 JSON 格式的推文數據\ndata = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nf.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 這部分請小心執行，這是 \"做過\" 雜訊處理的程式碼","metadata":{}},{"cell_type":"code","source":"import json\nimport re\nimport emoji\nimport pandas as pd\n\n# 定義表情符號到關鍵字的映射字典\nemoji_dict = {\n    '😂': '[joy]',\n    '❤️': '[love]',\n    '😍': '[adoration]',\n    '😭': '[cry]',\n    '❤': '[care]',\n    '😊': '[happy]',\n    '🙏': '[pray]',\n    '😘': '[kiss]',\n    '💕': '[love_each_other]',\n    '🔥': '[fire]',\n    '😩': '[weary]',\n    '🤔': '[think]',\n    '💯': '[perfect]',\n    '💙': '[loyalty]',\n    '🙄': '[annoyed]',\n    '😁': '[happy]',\n    '🙌': '[celebrate]',\n    '🙏🏾': '[pray]',\n    '👍': '[approve]',\n    '🙏🏽': '[pray]'\n}\n\n# 定義清理推文文本的函數\ndef clean_tweet(text, emoji_dict):\n    # 將定義的表情符號替換為對應的關鍵詞\n    for emj, keyword in emoji_dict.items():\n        text = text.replace(emj, keyword)\n    # 移除其餘的表情符號\n    text = emoji.replace_emoji(text, replace='')\n    # 移除 <LH> 標籤\n    text = re.sub(r'<LH>', '', text)\n    # 移除多餘的空白字元\n    text = text.strip()\n    return text\n\n# 讀取推文資料\ndata1 = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data1.append(json.loads(line))\n\n# 處理每條推文並儲存結果\nprocessed_tweets = []\nfor entry in data1:\n    # 檢查 '_source' 和 'tweet' 是否存在於記錄中\n    if '_source' in entry and 'tweet' in entry['_source']:\n        tweet = entry['_source']['tweet']\n        # 檢查 'text' 是否存在於 'tweet' 中\n        if 'text' in tweet:\n            tweet_text = tweet['text']\n            cleaned_text = clean_tweet(tweet_text, emoji_dict)\n            # 創建處理後的推文記錄，保留 '_source' 和 'tweet'\n            processed_tweet = {\n                '_source': {\n                    'tweet': tweet.copy()\n                }\n            }\n            # 更新清理後的文本\n            processed_tweet['_source']['tweet']['text'] = cleaned_text\n            processed_tweets.append(processed_tweet)\n        else:\n            print(\"記錄中缺少 'text' 鍵\")\n    else:\n        print(\"記錄中缺少 '_source' 或 'tweet' 鍵\")\n\n# 將處理後的推文資料存儲為 JSON 檔案\nwith open('/kaggle/working/tweets_DM_filtered_1.json', 'w') as outfile:\n    json.dump(processed_tweets, outfile, ensure_ascii=False, indent=4)\n\n\n\n# 將處理後的資料轉換為 DataFrame\ndf_processed = pd.DataFrame(processed_tweets)\n\n# 定義輸出目錄和檔案名稱\noutput_dir = '/kaggle/working/'\noutput_file = 'tweets_DM_filtered_1.json'\n\n# 檢查並創建目錄（如果不存在）\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# 將處理後的 DataFrame 儲存為 JSON 檔案\noutput_path = os.path.join(output_dir, output_file)\ndf_processed.to_json(output_path, orient='records', lines=True, force_ascii=False)\n\n# 把 /kaggle/working/tweets_DM_filtered_1.json 載入成為 data , 可樣就可以跟原本程式合併\n\ndata = []\nwith open('/kaggle/working/tweets_DM_filtered_1.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\nf.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T15:56:32.119086Z","iopub.execute_input":"2024-11-23T15:56:32.119445Z","iopub.status.idle":"2024-11-23T16:01:55.961281Z","shell.execute_reply.started":"2024-11-23T15:56:32.119413Z","shell.execute_reply":"2024-11-23T16:01:55.960077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 以下就都跟之前一樣了","metadata":{}},{"cell_type":"code","source":"# 2. 讀取其他 CSV 文件\nemotion = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\ndata_identification = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n\n# ======== 數據處理 ========\n# 1. 從 JSON 數據提取 tweet_id, hashtags, 和文本內容\ndf = pd.DataFrame(data)\n_source = df['_source'].apply(lambda x: x['tweet'])\ndf = pd.DataFrame({\n    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n    'hashtags': _source.apply(lambda x: x['hashtags']),\n    'text': _source.apply(lambda x: x['text']),\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T16:10:48.050793Z","iopub.execute_input":"2024-11-23T16:10:48.051320Z","iopub.status.idle":"2024-11-23T16:11:01.464818Z","shell.execute_reply.started":"2024-11-23T16:10:48.051281Z","shell.execute_reply":"2024-11-23T16:11:01.463448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# 2. 合併資料集標籤 (train/test)\ndf = df.merge(data_identification, on='tweet_id', how='left')\n\n# 3. 將資料分為訓練集和測試集\ntrain_data = df[df['identification'] == 'train']\ntest_data = df[df['identification'] == 'test']\n\n# 4. 合併情緒標籤到訓練資料中\ntrain_data = train_data.merge(emotion, on='tweet_id', how='left')\n\n# 5. 移除訓練集中重複的文本\ntrain_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n\n# ======== 訓練數據準備 ========\n# 1. 抽取部分訓練數據（30% 抽樣）\ntrain_data_sample = train_data.sample(frac=0.3, random_state=42)\n\n# 2. 提取目標變數 (情緒) 和特徵變數 (文本)\ny_train_data = train_data_sample['emotion']\nX_train_data = train_data_sample.drop(['tweet_id', 'emotion', 'identification', 'hashtags'], axis=1)\n\n# 3. 訓練集與測試集劃分 (分層抽樣確保類別分佈一致)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_train_data, y_train_data, test_size=0.2, random_state=42, stratify=y_train_data\n)\n\n# ======== 文本特徵轉換 ========\n# 1. 使用 TF-IDF 將文本轉換為數值特徵\ntfidf = TfidfVectorizer(max_features=500)\nX = tfidf.fit_transform(X_train['text']).toarray()\nX_test = tfidf.transform(X_test['text'])\n\n# ======== 標籤編碼 ========\n# 1. 將情緒類別轉換為數值型\nle = LabelEncoder()\ny = le.fit_transform(y_train)\ny_test = le.transform(y_test)\n\n# ======== 模型訓練 ========\n# 1. 訓練隨機森林分類器\nclf = RandomForestClassifier()\nclf.fit(X, y)\n\n# ======== 模型評估 ========\n# 1. 預測測試集\ny_pred = clf.predict(X_test)\n\n# 2. 計算準確率\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n\n# ======== 測試數據預測 ========\n# 1. 準備測試數據的文本特徵\nX_test_data = test_data.drop(['tweet_id', 'identification', 'hashtags'], axis=1)\nX_test_data = tfidf.transform(X_test_data['text']).toarray()\n\n# 2. 預測測試數據情緒\ny_test_pred = clf.predict(X_test_data)\n\n# ======== 結果輸出 ========\n# 1. 將數值標籤轉換回文字標籤\ny_pred_labels = le.inverse_transform(y_test_pred)\n\n# 2. 保存提交檔案\nsubmission = pd.DataFrame({\n    'tweet_id': test_data['tweet_id'],\n    'emotion': y_pred_labels\n})\n\n# 先不產生 submission\n# submission.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T16:11:08.923409Z","iopub.execute_input":"2024-11-23T16:11:08.924496Z","iopub.status.idle":"2024-11-23T16:27:35.031860Z","shell.execute_reply.started":"2024-11-23T16:11:08.924450Z","shell.execute_reply":"2024-11-23T16:27:35.030052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T16:28:59.270037Z","iopub.execute_input":"2024-11-23T16:28:59.270524Z","iopub.status.idle":"2024-11-23T16:28:59.284416Z","shell.execute_reply.started":"2024-11-23T16:28:59.270481Z","shell.execute_reply":"2024-11-23T16:28:59.283268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 分析部分 : 混淆矩陣 , Classification Report\n\n# 1. TF-IDF + Randon_forest\n## 準確率紀錄 :\n### 未做雜訊處理前\n- Accuracy: 0.4799\n- Accuracy: 0.4808\n\n### 做過雜訊處理後\n- Accuracy: 0.4717\n- Accuracy: 0.4708","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# ======== 分析部分 ========\n\n# 1. 列出混淆矩陣\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 2. 畫出混淆矩陣\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 3 . Classification Report\nclass_report = classification_report(y_test, y_pred, target_names=le.classes_)\nprint(\"Classification Report:\")\nprint(class_report)\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 4. 印出 Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"\\n\", \"----\", \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T16:29:04.034011Z","iopub.execute_input":"2024-11-23T16:29:04.034478Z","iopub.status.idle":"2024-11-23T16:29:04.726109Z","shell.execute_reply.started":"2024-11-23T16:29:04.034440Z","shell.execute_reply":"2024-11-23T16:29:04.724556Z"}},"outputs":[],"execution_count":null}]}