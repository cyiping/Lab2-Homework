{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 6. LSTM + TensorFlow  Tokenizer\n\n| **方法**                  | **特徵工程**                | **模型**                    | **準確率預估** | **優勢**                                     | **劣勢**                                      | **GPU 支援**            |\r\n|--------------------------|----------------------------|-----------------------------|----------------|---------------------------------------------|----------------------------------------------|-------------------------|\r\n| **TF-IDF + 隨機森林**      | 稀疏特徵表示，詞頻與逆文檔頻率權重 | 隨機森林                    | 75%-82%       | 模型穩定性強，對噪聲和高維數據不敏感          | 無法處理非線性模式，對語義信息利用不足           | 不支持                  |\r\n| **TF-IDF + Boosting**      | 稀疏特徵表示，詞頻與逆文檔頻率權重 | XGBoost 或 LightGBM         | 78%-85%       | 擅長處理稀疏特徵，對錯分樣本有良好適應能力      | 訓練成本略高，需調參以達到最佳效果              | 支持（顯著加速，適合大數據集）|\r\n| **Word2Vec + 隨機森林**     | 詞嵌入，計算句向量平均值       | 隨機森林                    | 72%-80%       | 能結合詞嵌入語義特徵，提升語義捕捉能力          | 詞嵌入需預處理，隨機森林對非線性語義的處理有限     | 不支持                  |\r\n| **Word2Vec + CNN**         | 詞嵌入，保留語序            | 卷積神經網絡                | 75%-85%       | 捕捉局部語義特徵，對短文本效果佳               | 訓練需較多資源，對長文本效果有限               | 支持（顯著加速）         |\r\n| **BERT 嵌入 + Transformer**| 上下文語義嵌入，保留全局語義 | 預訓練 BERT 模型             | 85%-90%       | 能捕捉上下文語義，分類準確率最高               | 訓練和推理成本高，需要大量數據和資源支持         | 支持（必要，否則速度較慢） |\r\n| **Tokenizer + LSTM**       | 數字化文本序列，保留序列上下文 | 長短期記憶神經網絡           | 80%-88%       | 能捕捉文本序列特徵，適合時間序列或長文本        | 訓練成本中等，對長文本可能有梯度消失問題         | 支持（顯著加速，必要）   |\r\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:52:32.548089Z","iopub.execute_input":"2024-11-24T13:52:32.548431Z","iopub.status.idle":"2024-11-24T13:52:32.556374Z","shell.execute_reply.started":"2024-11-24T13:52:32.548402Z","shell.execute_reply":"2024-11-24T13:52:32.555653Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json\n/kaggle/input/dm-2024-isa-5810-lab-2-homework/sampleSubmission.csv\n/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv\n/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 資料處理部分（保持原樣）\nimport json\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\n# 載入 JSON 數據\ndata = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\n\n# 載入情緒標籤與數據標識檔案\nemotion = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\ndata_identification = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n\n# 處理 JSON 數據，提取必要欄位\ndf = pd.DataFrame(data)\n_source = df['_source'].apply(lambda x: x['tweet'])\ndf = pd.DataFrame({\n    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n    'hashtags': _source.apply(lambda x: x['hashtags']),\n    'text': _source.apply(lambda x: x['text']),\n})\ndf = df.merge(data_identification, on='tweet_id', how='left')\n\n# 分為訓練和測試數據\ntrain_data = df[df['identification'] == 'train']\ntest_data = df[df['identification'] == 'test']\n\n# 合併情緒標籤\ntrain_data = train_data.merge(emotion, on='tweet_id', how='left')\n\n# 移除重複文本\ntrain_data.drop_duplicates(subset=['text'], keep=False, inplace=True)\n\n# 設定 Tokenizer，將文字轉為數字序列\nMAX_VOCAB_SIZE = 5000  # 最大詞彙數量\nMAX_SEQUENCE_LENGTH = 100  # 序列長度\n\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<UNK>\")\ntokenizer.fit_on_texts(train_data['text'])\n\n# 將文字轉為數字序列並進行填充\nX_train_sequences = tokenizer.texts_to_sequences(train_data['text'])\nX_test_sequences = tokenizer.texts_to_sequences(test_data['text'])\nX_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\nX_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n\n# 標籤編碼\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(train_data['emotion'])\n\n# 轉為 One-Hot 編碼格式\ny_train_onehot = to_categorical(y_train_encoded)\n\n# 使用 MirroredStrategy 進行多 GPU 訓練\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n\n# 建立分布式策略\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n\nwith strategy.scope():\n    # 嵌入層參數\n    EMBEDDING_DIM = 100  # 嵌入層維度\n    NUM_CLASSES = len(le.classes_)  # 類別數量\n\n    # 建立 LSTM 模型\n    model = Sequential([\n        Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n        LSTM(128, return_sequences=False),  # LSTM 層，128 個隱藏神經元\n        Dropout(0.5),  # Dropout，減少過擬合\n        Dense(64, activation='relu'),  # 隱藏層\n        Dropout(0.5),  # 再次 Dropout\n        Dense(NUM_CLASSES, activation='softmax')  # 輸出層，使用 Softmax 進行多分類\n    ])\n\n    # 編譯模型\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 顯示模型結構\nmodel.summary()\n\n# 模型訓練\nhistory = model.fit(\n    X_train_padded, y_train_onehot,\n    validation_split=0.2,  # 使用部分數據作為驗證集\n    epochs=10,  # 訓練輪數\n    batch_size=64,  # 批次大小（分布式策略下會自動分配到多 GPU）\n    verbose=1  # 顯示訓練過程\n)\n\n# 預測與提交\ny_test_pred = model.predict(X_test_padded)\ny_pred_labels = le.inverse_transform(y_test_pred.argmax(axis=1))  # 將預測轉回情緒標籤\n\nsubmission = pd.DataFrame({\n    'tweet_id': test_data['tweet_id'],\n    'emotion': y_pred_labels\n})\n# submission.to_csv('/kaggle/working/submission.csv', index=False)  # 儲存為提交檔案\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:52:35.547336Z","iopub.execute_input":"2024-11-24T13:52:35.548150Z","iopub.status.idle":"2024-11-24T14:40:13.984836Z","shell.execute_reply.started":"2024-11-24T13:52:35.548118Z","shell.execute_reply":"2024-11-24T14:40:13.983800Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 15ms/step - accuracy: 0.3528 - loss: 1.8009 - val_accuracy: 0.3544 - val_loss: 1.7879\nEpoch 2/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 14ms/step - accuracy: 0.3540 - loss: 1.7881 - val_accuracy: 0.3544 - val_loss: 1.7878\nEpoch 3/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 14ms/step - accuracy: 0.3548 - loss: 1.7865 - val_accuracy: 0.3544 - val_loss: 1.7877\nEpoch 4/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 14ms/step - accuracy: 0.3543 - loss: 1.7875 - val_accuracy: 0.3544 - val_loss: 1.7877\nEpoch 5/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 15ms/step - accuracy: 0.3548 - loss: 1.7877 - val_accuracy: 0.3544 - val_loss: 1.7877\nEpoch 6/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 15ms/step - accuracy: 0.3542 - loss: 1.7872 - val_accuracy: 0.3544 - val_loss: 1.7879\nEpoch 7/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 15ms/step - accuracy: 0.3540 - loss: 1.7880 - val_accuracy: 0.3544 - val_loss: 1.7877\nEpoch 8/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 14ms/step - accuracy: 0.3541 - loss: 1.7873 - val_accuracy: 0.3544 - val_loss: 1.7877\nEpoch 9/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 14ms/step - accuracy: 0.3549 - loss: 1.7871 - val_accuracy: 0.3544 - val_loss: 1.7877\nEpoch 10/10\n\u001b[1m18115/18115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 15ms/step - accuracy: 0.3546 - loss: 1.7872 - val_accuracy: 0.3544 - val_loss: 1.7877\n\u001b[1m12875/12875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 9ms/step\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"submission.head","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:42:27.818096Z","iopub.execute_input":"2024-11-24T13:42:27.818979Z","iopub.status.idle":"2024-11-24T13:42:27.831399Z","shell.execute_reply.started":"2024-11-24T13:42:27.818942Z","shell.execute_reply":"2024-11-24T13:42:27.830464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:50:44.375159Z","iopub.execute_input":"2024-11-24T13:50:44.375823Z","iopub.status.idle":"2024-11-24T13:50:44.388067Z","shell.execute_reply.started":"2024-11-24T13:50:44.375791Z","shell.execute_reply":"2024-11-24T13:50:44.387322Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   tweet_id       emotion\n0  0x3140b1       sadness\n1  0x368b73       disgust\n2  0x296183  anticipation\n3  0x2bd6e1           joy\n4  0x2ee1dd  anticipation","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0x3140b1</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0x368b73</td>\n      <td>disgust</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0x296183</td>\n      <td>anticipation</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0x2bd6e1</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0x2ee1dd</td>\n      <td>anticipation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# 確保 test_data 中包含真實標籤\nif 'emotion' not in test_data.columns:\n    raise ValueError(\"test_data 中缺少 'emotion' 欄位，無法進行分析。請確保 test_data 包含真實標籤。\")\n\n# 編碼測試集的真實標籤\ny_test_encoded = le.transform(test_data['emotion'])\ny_pred_encoded = y_test_pred.argmax(axis=1)  # 預測的數值標籤\n\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1. 列出混淆矩陣\nconf_matrix = confusion_matrix(y_test_encoded, y_pred_encoded)\nprint(\"\\n\", \"----\", \"\\n\")\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\n# 2. 畫出混淆矩陣\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 3. 印出 Classification Report\nclass_report = classification_report(y_test_encoded, y_pred_encoded, target_names=le.classes_)\nprint(\"Classification Report:\\n\", class_report)\n\nprint(\"\\n\", \"----\", \"\\n\")\n\n# 4. 印出 Accuracy\naccuracy = accuracy_score(y_test_encoded, y_pred_encoded)\nprint(\"Accuracy:\", accuracy)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:44:55.256140Z","iopub.execute_input":"2024-11-24T13:44:55.256977Z","iopub.status.idle":"2024-11-24T13:44:55.289259Z","shell.execute_reply.started":"2024-11-24T13:44:55.256945Z","shell.execute_reply":"2024-11-24T13:44:55.288096Z"}},"outputs":[],"execution_count":null}]}